{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some Preprocessing and cresting required files. Do not run the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in nltk.corpus.gutenberg.fileids():\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for file in nltk.corpus.gutenberg.fileids():\n",
    "#or iterate through the fields manually as\n",
    "sentences2 = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "sentences1 = [sent for sent in sentences2]\n",
    "sentences = [tagger.tag(sent) for sent in sentences1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"dictwPOS.txt\",\"a\")\n",
    "for sent in sentences:\n",
    "    for i in sent:\n",
    "        if str(i[0]) in vocab :\n",
    "            #print(\"skip  \")\n",
    "            continue\n",
    "        elif str(i[0]).isalpha() :\n",
    "            fName = str(i[1])\n",
    "            fName = fName +\".txt\"\n",
    "            f = open(fName,\"a\")\n",
    "            vocab.add(str(i[0]))\n",
    "            #print(len(vocab))\n",
    "            #print(\"%s\\n%s\\n\" % (str(i[0]),str(i[1])))\n",
    "            f.write(\"%s \" % (str(i[0])))\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import re\n",
    "FP = open(\"productionsGG.txt\",\"a\")\n",
    "dont = 0\n",
    "    \n",
    "for i in range(1501,1601):\n",
    "    ruleset = set()\n",
    "    ruleset.update(rule for tree in nltk.corpus.treebank.parsed_sents()[i] \n",
    "               for rule in tree.productions())\n",
    "\n",
    "    for rule in ruleset:\n",
    "        tokens = [str(rule).split(\" \")]\n",
    "        for j in tokens:\n",
    "            if dont == 0:\n",
    "                for x in j:\n",
    "                    for y in x:\n",
    "                        #print(y)\n",
    "                        if y == \"'\" :\n",
    "                            print(\"not adding \",rule)\n",
    "                            dont = 1\n",
    "                            break\n",
    "                        else:\n",
    "                            continue\n",
    "        if dont != 1:\n",
    "            print(\"adding\\n\",rule)\n",
    "            FP.write(str(rule))\n",
    "            FP.write(\"\\t\")\n",
    "        else:\n",
    "            dont = 0\n",
    "    print(len(ruleset))\n",
    "    FP.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enter your input: \n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " this is a good programm. it provides       accurete corrections.\n"
     ]
    }
   ],
   "source": [
    "print(\"enter your input: \\n\")\n",
    "text = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]     C:\\Users\\Monica Gowda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_treebank_pos_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Run once cell #\n",
    "\n",
    "import nltk\n",
    "nltk.download('maxent_treebank_pos_tagger')\n",
    "tagger = nltk.data.load('taggers/maxent_treebank_pos_tagger/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once cell #\n",
    "\n",
    "import spacy\n",
    "from gensim.models.keyedvectors import WordEmbeddingsKeyedVectors\n",
    "import en_core_web_lg\n",
    "nlp = en_core_web_lg.load()\n",
    "#nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once cell #\n",
    "\n",
    "wordList =[]\n",
    "vectorList = []\n",
    "for key, vector in nlp.vocab.vectors.items():\n",
    "    try:\n",
    "        wordList.append(nlp.vocab.strings[key] )\n",
    "    except KeyError:\n",
    "        print(key)\n",
    "        continue\n",
    "    vectorList.append(vector)\n",
    "\n",
    "myV = WordEmbeddingsKeyedVectors(nlp.vocab.vectors_length)\n",
    "\n",
    "myV.add(wordList, vectorList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please check fro suggested spelling corrections:\n",
      "Check from the rightmost suggestion\n",
      "\n",
      "programm  :  ['promulge', 'progress', 'programme', 'program']\n",
      "accurete  :  ['accumulate', 'athlete', 'concrete', 'accurate']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#suggest spelling corrections for incorrectly spelt words\n",
    "\n",
    "#nltk.download('punkt')\n",
    "sent1 = nltk.word_tokenize(text)\n",
    "#dist = 10\n",
    "tokenized = tagger.tag(sent1)\n",
    "#print(tokenized)\n",
    "#F = open(\"dictwPOS.txt\",\"r\")\n",
    "#dic = str(F.read())\n",
    "#dic = str(dic.split(\"\\n\"))\n",
    "#count = 1\n",
    "\n",
    "print(\"please check fro suggested spelling corrections:\\nCheck from the rightmost suggestion\\n\")\n",
    "for x in tokenized:\n",
    "    if x[0].isalpha():\n",
    "        dist=10\n",
    "        correction = []\n",
    "        if x[1] == \"PRP$\":\n",
    "            fName=\"PRP\"\n",
    "        else:\n",
    "            fName = str(x[1])\n",
    "        fName = fName +\".txt\"\n",
    "        f = open(fName,\"r\")\n",
    "        dictp = [f.read()]\n",
    "        for word in nltk.word_tokenize(str(dictp)):\n",
    "            #print(word)\n",
    "            if nltk.edit_distance(x[0],word) <= dist and nltk.edit_distance(x[0],word)< len(x[0]):\n",
    "                #print(word[1],word[0])\n",
    "                correction = correction + [word]\n",
    "                dist = nltk.edit_distance(x[0],word)\n",
    "        f.close()\n",
    "        if str(x[0]) == correction[-1]:\n",
    "            continue\n",
    "        else:\n",
    "            print(x[0],\" : \",correction[-4:])\n",
    "print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to correct your text now?\n",
      "\n",
      "If yes, press \"Y\". Else press any character\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n",
      " this is a good program. it provides       accurate corrections.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Text has been updated.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#apply the suggested changes you would like to make.\n",
    "\n",
    "print(\"do you want to correct your text now?\\n\\nIf yes, press \\\"Y\\\". Else press any character\")\n",
    "yn = input()\n",
    "if yn== \"Y\" or yn ==\"y\":\n",
    "    text = input()\n",
    "    prev = text\n",
    "    print(\"\\n\\nText has been updated.\\n\")\n",
    "else:\n",
    "    print(\"No changes made.\\nText remains as:\\n\\n\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "here are some word replacement suggestions:\n",
      "\n",
      "good : great \n",
      "\n",
      "accurate : Precise \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Retrieve alternate word suggestions\n",
    "\n",
    "tags = ['JJ','JJR','JJS','RB','RBR','RBS',]\n",
    "resDist = 0\n",
    "suggSet = set()\n",
    "\n",
    "sentences1 = nltk.sent_tokenize(text)\n",
    "sentences1 = [nltk.word_tokenize(sent) for sent in sentences1]\n",
    "#sentences = [nltk.pos_tag(sent) for sent in sentences1]\n",
    "sentences = [tagger.tag(sent) for sent in sentences1]#adding postag for words\n",
    "#print(sentences)\n",
    "for line in sentences:\n",
    "    #print(line)\n",
    "    for word in line:\n",
    "        if(word[1] in tags):\n",
    "            try:\n",
    "                sugg = myV.most_similar(word[0], topn=5)\n",
    "            except KeyError:\n",
    "                print(\"\\n\")\n",
    "                continue\n",
    "            for x in sugg:\n",
    "                if nltk.edit_distance(x[0], word[0]) > resDist:\n",
    "                    res = x[0]\n",
    "                    resdist = nltk.edit_distance(x[0], word[0])\n",
    "            add = str(word[0]) + \" : \" + res\n",
    "            suggSet.add(add)\n",
    "print(\"here are some word replacement suggestions:\\n\")\n",
    "for sugg in suggSet:\n",
    "    print(sugg,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to correct your text now?\n",
      "If yes, press \"Y\". else press any character\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no changes made\n",
      " this is a good program. it provides       accurate corrections.\n"
     ]
    }
   ],
   "source": [
    "print(\"do you want to correct your text now?\\nIf yes, press \\\"Y\\\". else press any character\")\n",
    "yn = input()\n",
    "if yn== \"Y\" or yn ==\"y\":\n",
    "    text = input()\n",
    "    prev = text\n",
    "    print(\"\\n\\ninput data changed from: \",prev,\"\\nTo :\",text)\n",
    "else:\n",
    "    print(\"no changes made\\n\",text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a good program\n",
      "sentence matches my grammar\n",
      "\n",
      "it provides       accurate corrections.\n",
      "check the grammar\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check for correctness of grammatic syntax.\n",
    "\n",
    "from nltk.parse.generate import generate\n",
    "from nltk import CFG\n",
    "FP = open(\"productionsGG.txt\",\"r\")\n",
    "#sent = (\"this is a GrammarChecker. it checks for grammatical errors in text. no parse this sentence. the previous sentence is wrong. this test no good. the previous sentence was worng. the results are not good. this test is not good\")\n",
    "sentences = text.split(\". \")\n",
    "for sent1 in sentences:\n",
    "    sent = tagger.tag(nltk.word_tokenize(sent1))\n",
    "    #print(sent)\n",
    "    puncts = [\".\",\",\",\"!\",\"?\"]\n",
    "    prod=[]\n",
    "    for word in sent:\n",
    "        if word[0] in puncts:\n",
    "            continue\n",
    "        else:\n",
    "            if word[1] == 'PRP$':\n",
    "                prod = prod + [\"PRP\" + \" -> '\" + str(word[0]) + \"'\\t\"]\n",
    "            else:\n",
    "                prod = prod + [str(word[1]) + \" -> '\" + str(word[0]) + \"'\\t\"]\n",
    "    #print(prod)\n",
    "    print(sent1)\n",
    "\n",
    "\n",
    "    result = \"no\"\n",
    "\n",
    "    FG = open(\"productionsGG.txt\",\"r\")\n",
    "    file = FG.read()\n",
    "    file = file.split(\"\\n\")\n",
    "    #my_grammar = ['S -> NP VP\\t']\n",
    "\n",
    "    for line in file:\n",
    "        count=0\n",
    "        my_grammar = ['S -> NP vp\\t','S -> NP','S -> VP',\"CM -> ','\" ,\"FS -> '.'\",\"CLN -> ':'\",\"DOL -> '$'\"]\n",
    "        my_grammar = my_grammar + str(line).split(\"\\t\")\n",
    "        for x in prod:\n",
    "            my_grammar = my_grammar + [str(x)]\n",
    "            #print(my_grammar)\n",
    "        try:\n",
    "            grammar = CFG.fromstring(my_grammar)\n",
    "            #print(grammar)\n",
    "            FP.close\n",
    "            grammar.start()\n",
    "            sent = sent1.split()\n",
    "\n",
    "            parser = nltk.ChartParser(grammar)\n",
    "    \n",
    "            for tree in parser.parse(sent):\n",
    "                #print(grammar)\n",
    "                #print(tree)\n",
    "                result = \"yes\"\n",
    "                break\n",
    "        except ValueError:\n",
    "            count= count+1\n",
    "            #print(count)\n",
    "            \n",
    "    if result == \"no\":\n",
    "        print(\"check the grammar\\n\")\n",
    "    else :\n",
    "        print(\"sentence matches my grammar\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you want to apply some auto correction?? Enter \"Y\" for yes.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "This is a good program. It provides accurate corrections \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Autocapitalise and remove additional white spaces.\n",
    "\n",
    "print(\"do you want to apply some auto correction?? Enter \\\"Y\\\" for yes.\")\n",
    "yn=input()\n",
    "doc1 = str()\n",
    "if yn ==\"y\" or yn == \"Y\":\n",
    "    text = ' '.join(text.split())\n",
    "    doc = nlp(text)\n",
    "    print(\"\\n\")\n",
    "    for ent in doc.ents:\n",
    "        if ent.label == \"GPE\":\n",
    "            text[ent.start_char].toupper()\n",
    "    for token in doc:\n",
    "        if str(doc[token.i-1]) == \".\":\n",
    "            #print(token)\n",
    "            doc1 = doc1 + str(token).capitalize() + \" \"\n",
    "            #print(doc[token.i])\n",
    "            #token.start_char.toupper()\n",
    "        elif token.i+1 <= len(doc)-1:\n",
    "            if  str(doc[token.i+1]) == \".\" :\n",
    "                doc1 = doc1 + str(token)\n",
    "            else :\n",
    "                doc1 = doc1 + str(token) + \" \"\n",
    "    text = str(doc1)\n",
    "    print(text,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
